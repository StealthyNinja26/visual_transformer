# 🔍 Visual Transformer Implementation

This project explores the implementation and understanding of **Vision Transformers (ViTs)** for image classification tasks using the CIFAR-10 dataset. The notebook walks through building a transformer-based model architecture for computer vision from scratch using PyTorch.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]((https://colab.research.google.com/drive/1AEYjMUBTrL4YLepzxraBX0ick_pYyaFM?usp=sharing))

---

## 📁 Contents

- `Visual_transformer.ipynb` – Main notebook with the full implementation.
- CIFAR-10 Dataset – Downloaded automatically via torchvision.
- Visualization tools and training logs embedded in the notebook.

---

## 📌 Features

- ✅ Custom implementation of Vision Transformer (ViT) architecture.
- ✅ Patch embedding and positional encoding mechanisms.
- ✅ Multi-head self-attention and transformer encoder blocks.
- ✅ Model training and evaluation on CIFAR-10.
- ✅ Visualization of training loss and accuracy.

---

## 🚀 Getting Started

1. Open the notebook in [Google Colab](https://colab.research.google.com).
2. Connect to the T4 GPU or any other runtime with a GPU.
3. Run all cells (`Runtime > Run all`) to:
   - Load CIFAR-10 dataset.
   - Initialize and train the Vision Transformer model.
   - Evaluate the model and visualize results.

---

## 🧰 Requirements

All required dependencies are handled inside the Colab notebook. Main libraries used:

- `torch`
- `torchvision`
- `matplotlib`
- `numpy`

No manual installation is needed when using Colab.

---

## 📊 Results

The notebook includes:
- Accuracy and loss curves
- Final test accuracy
- Model architecture summary

---
