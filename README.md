# ðŸ” Visual Transformer Implementation

This project explores the implementation and understanding of **Vision Transformers (ViTs)** for image classification tasks using the CIFAR-10 dataset. The notebook walks through building a transformer-based model architecture for computer vision from scratch using PyTorch.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]((https://colab.research.google.com/drive/1AEYjMUBTrL4YLepzxraBX0ick_pYyaFM?usp=sharing))

---

## ðŸ“ Contents

- `Visual_transformer.ipynb` â€“ Main notebook with the full implementation.
- CIFAR-10 Dataset â€“ Downloaded automatically via torchvision.
- Visualization tools and training logs embedded in the notebook.

---

## ðŸ“Œ Features

- âœ… Custom implementation of Vision Transformer (ViT) architecture.
- âœ… Patch embedding and positional encoding mechanisms.
- âœ… Multi-head self-attention and transformer encoder blocks.
- âœ… Model training and evaluation on CIFAR-10.
- âœ… Visualization of training loss and accuracy.

---

## ðŸš€ Getting Started

1. Open the notebook in [Google Colab](https://colab.research.google.com).
2. Connect to the T4 GPU or any other runtime with a GPU.
3. Run all cells (`Runtime > Run all`) to:
   - Load CIFAR-10 dataset.
   - Initialize and train the Vision Transformer model.
   - Evaluate the model and visualize results.

---

## ðŸ§° Requirements

All required dependencies are handled inside the Colab notebook. Main libraries used:

- `torch`
- `torchvision`
- `matplotlib`
- `numpy`

No manual installation is needed when using Colab.

---

## ðŸ“Š Results

The notebook includes:
- Accuracy and loss curves
- Final test accuracy
- Model architecture summary

---
